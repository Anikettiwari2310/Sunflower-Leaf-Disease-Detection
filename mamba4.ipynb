{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9297cb-0116-48a7-89be-fbf78d1ed965",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install vision-mamba\n",
    "!pip install datasets\n",
    "!unzip /content/Original_Image.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cc470-455d-4e93-b580-ae50a58d5e5f",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffcb6a2-2b8e-4e9e-aef0-f46eccbeee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vision_mamba import Vim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641bda8-959b-42ed-ac9e-e21f6c911679",
   "metadata": {},
   "source": [
    "# Define Transformations and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c23464-b1b7-4ce2-bdb7-e1a77494c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (resize images, convert to tensors, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = r\"C:\\Users\\aweso\\OneDrive - National Institute of Technology\\NITT\\Semesters\\Sem 8\\FYP\\Project\\Sunflower\\Original_Image\\Original Image\"  # Update this path\n",
    "\n",
    "# Load dataset using ImageFolder\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Get class names\n",
    "print(f\"Classes: {dataset.classes}\")\n",
    "num_classes = len(dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36508d0b-35be-4fc1-98e5-7891fc243625",
   "metadata": {},
   "source": [
    "# Split Dataset into Train, Validation, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0739b-1210-4ab1-9927-a64d2c26af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset split sizes\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size  # Ensures the split is exact\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd0691-432c-4066-94f5-7416b59db4f0",
   "metadata": {},
   "source": [
    "# Define Teacher and Student Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a081daa-e45d-4bf9-9928-85792994b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper neural network class to be used as teacher (Mamba-based)\n",
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes=4):  # Update num_classes to 4\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.model = Vim(\n",
    "            dim=256,          # Dimension of Mamba model\n",
    "            dt_rank=32,       # Mamba SSM rank\n",
    "            dim_inner=256,    # Inner dimension\n",
    "            d_state=256,      # State dimension\n",
    "            num_classes=num_classes,  # Number of output classes\n",
    "            image_size=224,   # Input image size\n",
    "            patch_size=16,    # Patch size\n",
    "            channels=3,       # RGB images\n",
    "            dropout=0.1,      # Regularization\n",
    "            depth=12          # Teacher model has 12 layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Lightweight neural network class to be used as student (Mamba-based)\n",
    "class LightNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes=4):  # Update num_classes to 4\n",
    "        super(LightNN, self).__init__()\n",
    "        self.model = Vim(\n",
    "            dim=256,          # Dimension of Mamba model\n",
    "            dt_rank=32,       # Mamba SSM rank\n",
    "            dim_inner=256,    # Inner dimension\n",
    "            d_state=256,      # State dimension\n",
    "            num_classes=num_classes,  # Number of output classes\n",
    "            image_size=224,   # Input image size\n",
    "            patch_size=16,    # Patch size\n",
    "            channels=3,       # RGB images\n",
    "            dropout=0.1,      # Regularization\n",
    "            depth=6           # Student model has fewer layers (6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ðŸ”¹ Initialize the models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "teacher_model = DeepNN(num_classes=4).to(device)\n",
    "student_model = LightNN(num_classes=4).to(device)\n",
    "\n",
    "# ðŸ”¹ Print model summaries\n",
    "print(\"Teacher Model (DeepNN):\")\n",
    "print(teacher_model)\n",
    "\n",
    "print(\"\\nStudent Model (LightNN):\")\n",
    "print(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cf999-e7be-48e1-8e07-16b0989011af",
   "metadata": {},
   "source": [
    "# Define Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be67862-4c19-4fe6-94cd-f14e2ca87981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240acc8-67d0-470e-acfd-b295eb947518",
   "metadata": {},
   "source": [
    "# Train and Test Teacher and Student Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b454bf-a3ca-404f-8b3b-c5a127bf09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "nn_deep = DeepNN(num_classes=4).to(device)\n",
    "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
    "\n",
    "# Instantiate the lightweight network:\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbc50a-9021-4062-964b-20c35999c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9fc08-9ddd-41f0-9cee-e5c91099e274",
   "metadata": {},
   "source": [
    "# Check Norms of First Layer Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bedf6c-ec02-4f8f-a9f6-140644731523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the norm of the first layer of the initial lightweight model\n",
    "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
    "# Print the norm of the first layer of the new lightweight model\n",
    "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3ca64-fad6-4a59-b0c6-21feeda59c87",
   "metadata": {},
   "source": [
    "# Print Model Parameter Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6836f33-8711-46fd-869a-ff2dc0aa36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_light}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b145d-1a7f-45c5-8ffd-896f65cb4f38",
   "metadata": {},
   "source": [
    "# Train and Test Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0552b2a-2dd1-44ce-b415-4c0ba0ca76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_light_ce = test(nn_light, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10563a-7963-4818-a8fc-9384443ffcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71be91a-63c8-45a4-82c1-0e28052601c0",
   "metadata": {},
   "source": [
    "# Train with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d30a1b-9c7f-4290-ba63-c08dac578f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a078e-61ec-4279-a020-17cacc1d530b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
